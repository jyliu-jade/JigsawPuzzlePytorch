{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liujiayao/anaconda3/envs/nlp_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/liujiayao/anaconda3/envs/nlp_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/liujiayao/anaconda3/envs/nlp_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/liujiayao/anaconda3/envs/nlp_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/liujiayao/anaconda3/envs/nlp_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/liujiayao/anaconda3/envs/nlp_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Sep 14 12:16:31 2017\n",
    "\n",
    "@author: Biagio Brattoli\n",
    "\"\"\"\n",
    "import os, sys, numpy as np\n",
    "import argparse\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow # needs to call tensorflow before torch, otherwise crush\n",
    "sys.path.append('Utils')\n",
    "from logger import Logger\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "sys.path.append('Dataset')\n",
    "#from JigsawNetwork import Network\n",
    "from JigsawNetwork_darknet import Network\n",
    "from TrainingUtils import adjust_learning_rate, compute_accuracy\n",
    "\n",
    "#from ImageDataLoader import DataLoader\n",
    "from JigsawImageLoader import JigsawDataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_scene_index = np.arange(0,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "write() takes exactly one argument (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e709a8b2c953>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'jigsaw_val.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}_{}.jpg'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscene\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: write() takes exactly one argument (2 given)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# training\n",
    "train_scene_index = np.arange(95)\n",
    "val_scene_index = np.arange(95,106)\n",
    "sample_index = np.arange(126)\n",
    "\n",
    "# training\n",
    "for scene in train_scene_index:\n",
    "    for sample in sample_index:\n",
    "        with open('jigsaw_train.txt', 'a') as f:\n",
    "            f.write('scene_{}/sample_{}.jpeg\\n'.format(scene, sample)) \n",
    "# validation\n",
    "val_scene_index = np.arange(95,106)\n",
    "for scene in val_scene_index:\n",
    "    for sample in sample_index:\n",
    "        with open('jigsaw_val.txt', 'a') as f:\n",
    "            f.write('scene_{}/sample_{}.jpeg\\n'.format(scene, sample)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation\n",
    "val_scene_index = np.arange(95,106)\n",
    "for scene in val_scene_index:\n",
    "    for sample in sample_index:\n",
    "        with open('jigsaw_val.txt', 'a') as f:\n",
    "            f.write('{}_{}.jpg\\n'.format(scene, sample)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([0,12])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainpath = '/Users/liujiayao/Desktop/1008 Deep Learning/Project/PyTorch-YOLOv3'\n",
    "train_data = JigsawDataset(trainpath,trainpath+'/data/custom/new_train.txt',\n",
    "                            classes=1000)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data,\n",
    "                                            batch_size=2,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=2)\n",
    "N = train_data.N\n",
    "    \n",
    "iter_per_epoch = train_data.N/2\n",
    "#print('Images: train %d, validation %d'%(train_data.N,val_data.N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.), tensor(0.)]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_accuracy(outputs, labels, topk=(1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 69, 346])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([2, 1000])\n",
      "0.0\n",
      "1\n",
      "torch.Size([2, 1000])\n",
      "0.0\n",
      "2\n",
      "torch.Size([2, 1000])\n",
      "0.0\n",
      "3\n",
      "torch.Size([2, 1000])\n",
      "0.0\n",
      "4\n",
      "torch.Size([2, 1000])\n",
      "0.0\n",
      "5\n",
      "torch.Size([2, 1000])\n",
      "0.0\n",
      "6\n",
      "torch.Size([2, 1000])\n",
      "0.0\n",
      "7\n",
      "torch.Size([2, 1000])\n",
      "0.0\n",
      "8\n",
      "torch.Size([2, 1000])\n",
      "0.0\n",
      "9\n",
      "torch.Size([2, 1000])\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "gpu = None\n",
    "net = Network(1000)\n",
    "if gpu is not None:\n",
    "    net.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(),lr=0.001,momentum=0.9,weight_decay = 5e-4)\n",
    "for i, (images, labels, original) in enumerate(train_loader):\n",
    "    print(i)\n",
    "    images = Variable(images)\n",
    "    labels = Variable(labels)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(images)\n",
    "    print(outputs.size())\n",
    "    prec1, prec5 = compute_accuracy(outputs, labels, topk=(1, 5))\n",
    "    acc = prec1.item()\n",
    "    print(acc)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.8644, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (backbone): DarkNet(\n",
       "    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu1): LeakyReLU(negative_slope=0.1)\n",
       "    (layer1): Sequential(\n",
       "      (ds_conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (ds_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (ds_relu): LeakyReLU(negative_slope=0.1)\n",
       "      (residual_0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1)\n",
       "        (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (ds_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (ds_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (ds_relu): LeakyReLU(negative_slope=0.1)\n",
       "      (residual_0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1)\n",
       "        (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "      (residual_1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1)\n",
       "        (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (ds_conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (ds_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (ds_relu): LeakyReLU(negative_slope=0.1)\n",
       "      (residual_0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1)\n",
       "        (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "      (residual_1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1)\n",
       "        (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "      (residual_2): BasicBlock(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1)\n",
       "        (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "      (residual_3): BasicBlock(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1)\n",
       "        (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "      (residual_4): BasicBlock(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1)\n",
       "        (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "      (residual_5): BasicBlock(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1)\n",
       "        (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "      (residual_6): BasicBlock(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1)\n",
       "        (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "      (residual_7): BasicBlock(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1)\n",
       "        (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (ds_conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (ds_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (ds_relu): LeakyReLU(negative_slope=0.1)\n",
       "      (residual_0): BasicBlock(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1)\n",
       "        (conv2): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "      (residual_1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1)\n",
       "        (conv2): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "      (residual_2): BasicBlock(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1)\n",
       "        (conv2): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "      (residual_3): BasicBlock(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1)\n",
       "        (conv2): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "      (residual_4): BasicBlock(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1)\n",
       "        (conv2): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "      (residual_5): BasicBlock(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1)\n",
       "        (conv2): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "      (residual_6): BasicBlock(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1)\n",
       "        (conv2): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "      (residual_7): BasicBlock(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1)\n",
       "        (conv2): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "    )\n",
       "    (layer5): Sequential(\n",
       "      (ds_conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (ds_bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (ds_relu): LeakyReLU(negative_slope=0.1)\n",
       "      (residual_0): BasicBlock(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1)\n",
       "        (conv2): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "      (residual_1): BasicBlock(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1)\n",
       "        (conv2): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "      (residual_2): BasicBlock(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1)\n",
       "        (conv2): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "      (residual_3): BasicBlock(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1)\n",
       "        (conv2): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv1_extra): Sequential(\n",
       "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (leakyrelu1): LeakyReLU(negative_slope=0.1)\n",
       "  )\n",
       "  (fc1_extra): Sequential(\n",
       "    (fc1): Linear(in_features=2304, out_features=1024, bias=True)\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (drop1): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (fc2_extra): Sequential(\n",
       "    (fc2): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (relu2): ReLU(inplace=True)\n",
       "    (drop2): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (fc3): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "        for i, (images, labels, original) in enumerate(train_loader):\n",
    "            batch_time.append(time()-end)\n",
    "            if len(batch_time)>100:\n",
    "                del batch_time[0]\n",
    "            \n",
    "            images = Variable(images)\n",
    "            labels = Variable(labels)\n",
    "            if args.gpu is not None:\n",
    "                images = images.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()\n",
    "            t = time()\n",
    "            outputs = net(images)\n",
    "            net_time.append(time()-t)\n",
    "            if len(net_time)>100:\n",
    "                del net_time[0]\n",
    "            \n",
    "            prec1, prec5 = compute_accuracy(outputs.cpu().data, labels.cpu().data, topk=(1, 5))\n",
    "            acc = prec1[0]\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss = float(loss.cpu().data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Sep 13 15:57:01 2017\n",
    "\n",
    "@author: Biagio Brattoli\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import cat\n",
    "import torch.nn.init as init\n",
    "\n",
    "import sys\n",
    "sys.path.append('Utils')\n",
    "from Layers import LRN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "from darknet import darknet53\n",
    "\n",
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self, classes=1000):\n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        self.backbone = darknet53('')\n",
    "        \n",
    "        self.conv1_extra = nn.Sequential()\n",
    "        self.conv1_extra.add_module('conv1', nn.Conv2d(1024, 256, 1, 1))\n",
    "        self.conv1_extra.add_module('bn1',nn.BatchNorm2d(256))\n",
    "        self.conv1_extra.add_module('leakyrelu1',nn.LeakyReLU(0.1))\n",
    "        \n",
    "        self.fc1_extra = nn.Sequential()\n",
    "        self.fc1_extra.add_module('fc1',nn.Linear(256*3*3, 1024))\n",
    "        self.fc1_extra.add_module('relu1',nn.ReLU(inplace=True))\n",
    "        self.fc1_extra.add_module('drop1',nn.Dropout(p=0.5))\n",
    "\n",
    "        self.fc2_extra = nn.Sequential()\n",
    "        self.fc2_extra.add_module('fc2',nn.Linear(9*1024,4096))\n",
    "        self.fc2_extra.add_module('relu2',nn.ReLU(inplace=True))\n",
    "        self.fc2_extra.add_module('drop2',nn.Dropout(p=0.5))\n",
    "\n",
    "        self.classifier = nn.Sequential()\n",
    "        self.classifier.add_module('fc3',nn.Linear(4096, classes))\n",
    "        \n",
    "        #self.apply(weights_init)\n",
    "\n",
    "    def load(self,checkpoint):\n",
    "        model_dict = self.state_dict()\n",
    "        pretrained_dict = torch.load(checkpoint)\n",
    "        pretrained_dict = {k: v for k, v in list(pretrained_dict.items()) if k in model_dict and 'fc8' not in k}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        self.load_state_dict(model_dict)\n",
    "        print([k for k, v in list(pretrained_dict.items())])\n",
    "\n",
    "    def save(self,checkpoint):\n",
    "        torch.save(self.state_dict(), checkpoint)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B,T,C,H,W = x.size()\n",
    "        x = x.transpose(0,1)\n",
    "\n",
    "        x_list = []\n",
    "        for i in range(9):\n",
    "            z = self.backbone(x[i])[-1]\n",
    "            z = self.conv1_extra(z)\n",
    "            z = self.fc1_extra(z.view(B,-1))\n",
    "            z = z.view([B,1,-1])\n",
    "            x_list.append(z)\n",
    "\n",
    "        x = cat(x_list,1)\n",
    "        x = self.fc2_extra(x.view(B,-1))\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def weights_init(model):\n",
    "    if type(model) in [nn.Conv2d,nn.Linear]:\n",
    "        nn.init.xavier_normal(model.weight.data)\n",
    "        nn.init.constant(model.bias.data, 0.1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0163, -0.0098,  0.0119,  ...,  0.0107, -0.0042,  0.0112],\n",
       "        [ 0.0138,  0.0002, -0.0007,  ..., -0.0020, -0.0113, -0.0035]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from JigsawNetwork import Network\n",
    "net = Network(1000)\n",
    "net(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0405, -0.0044,  0.0397,  ...,  0.0684, -0.0969,  0.0366],\n",
       "        [ 0.0774, -0.2742,  0.1228,  ..., -0.1444,  0.0617,  0.0547]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = darknet53('')\n",
    "x = images\n",
    "B,T,C,H,W = x.size()\n",
    "x = x.transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_list = []\n",
    "for i in range(9):\n",
    "    z = backbone(x[i])[-1]\n",
    "    z = conv1(z)\n",
    "    z = fc1(z.view(B,-1))\n",
    "    z = z.view([B,1,-1])\n",
    "    x_list.append(z)\n",
    "x = cat(x_list,1)\n",
    "x = fc2(x.view(B,-1))\n",
    "x = classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1000])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1024])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc1(z.view(B,-1)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = nn.Sequential()\n",
    "conv1.add_module('conv1', nn.Conv2d(1024, 256, 1, 1))\n",
    "conv1.add_module('bn1',nn.BatchNorm2d(256))\n",
    "conv1.add_module('relu1',nn.LeakyReLU(0.1))\n",
    "fc1 = nn.Sequential()\n",
    "fc1.add_module('fc1_s1',nn.Linear(256*3*3, 1024))\n",
    "fc1.add_module('relu6_s1',nn.ReLU(inplace=True))\n",
    "fc1.add_module('drop6_s1',nn.Dropout(p=0.5))\n",
    "\n",
    "fc2 = nn.Sequential()\n",
    "fc2.add_module('fc2',nn.Linear(9*1024,4096))\n",
    "fc2.add_module('relu2',nn.ReLU(inplace=True))\n",
    "fc2.add_module('drop2',nn.Dropout(p=0.5))\n",
    "\n",
    "classifier = nn.Sequential()\n",
    "classifier.add_module('fc3',nn.Linear(4096, classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Sequential(OrderedDict([\n",
    "            (\"conv\", nn.Conv2d(_in, _out, kernel_size=ks, stride=1, padding=pad, bias=False)),\n",
    "            (\"bn\", nn.BatchNorm2d(_out)),\n",
    "            (\"relu\", nn.LeakyReLU(0.1)),\n",
    "        ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 9216])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.view(B,-1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2304"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "256*3*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256, 10, 10])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512, 5, 5])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1024, 3, 3])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[2].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        self.fc6 = nn.Sequential()\n",
    "        self.fc6.add_module('fc6_s1',nn.Linear(256*3*3, 1024))\n",
    "        self.fc6.add_module('relu6_s1',nn.ReLU(inplace=True))\n",
    "        self.fc6.add_module('drop6_s1',nn.Dropout(p=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B,T,C,H,W = x.size()\n",
    "x_list = []\n",
    "for i in range(9):\n",
    "        z = self.conv(x[i])\n",
    "            z = self.fc6(z.view(B,-1))\n",
    "            z = z.view([B,1,-1])\n",
    "            x_list.append(z)\n",
    "\n",
    "net = Network(args.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [32, 3, 3, 3], but got 5-dimensional input of size [2, 9, 3, 75, 75] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-b467d71ae2fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/1008 Deep Learning/Project/Darknet/JigsawPuzzlePytorch/darknet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    344\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    345\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 346\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [32, 3, 3, 3], but got 5-dimensional input of size [2, 9, 3, 75, 75] instead"
     ]
    }
   ],
   "source": [
    "backbone(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "from .backbone import backbone_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, numpy as np\n",
    "import argparse\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow # needs to call tensorflow before torch, otherwise crush\n",
    "sys.path.append('Utils')\n",
    "from logger import Logger\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "sys.path.append('Dataset')\n",
    "from JigsawNetwork import Network\n",
    "\n",
    "from TrainingUtils import adjust_learning_rate, compute_accuracy\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Train JigsawPuzzleSolver on Imagenet')\n",
    "parser.add_argument('data', type=str, help='Path to Imagenet folder')\n",
    "parser.add_argument('--model', default=None, type=str, help='Path to pretrained model')\n",
    "parser.add_argument('--classes', default=1000, type=int, help='Number of permutation to use')\n",
    "parser.add_argument('--gpu', default=0, type=int, help='gpu id')\n",
    "parser.add_argument('--epochs', default=70, type=int, help='number of total epochs for training')\n",
    "parser.add_argument('--iter_start', default=0, type=int, help='Starting iteration count')\n",
    "parser.add_argument('--batch', default=256, type=int, help='batch size')\n",
    "parser.add_argument('--checkpoint', default='checkpoints/', type=str, help='checkpoint folder')\n",
    "parser.add_argument('--lr', default=0.001, type=float, help='learning rate for SGD optimizer')\n",
    "parser.add_argument('--cores', default=0, type=int, help='number of CPU core for loading')\n",
    "parser.add_argument('-e', '--evaluate', dest='evaluate', action='store_true',\n",
    "                    help='evaluate model on validation set, No training')\n",
    "args = parser.parse_args()\n",
    "\n",
    "#from ImageDataLoader import DataLoader\n",
    "from JigsawImageLoader import DataLoader\n",
    "\n",
    "\n",
    "def main():\n",
    "    if args.gpu is not None:\n",
    "        print(('Using GPU %d'%args.gpu))\n",
    "        os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(args.gpu)\n",
    "    else:\n",
    "        print('CPU mode')\n",
    "    \n",
    "    print('Process number: %d'%(os.getpid()))\n",
    "    \n",
    "    ## DataLoader initialize ILSVRC2012_train_processed\n",
    "    trainpath = '/Users/liujiayao/Desktop/1008 Deep Learning/Project/PyTorch-YOLOv3'\n",
    "    if os.path.exists(trainpath+'_255x255'):\n",
    "        trainpath += '_255x255'\n",
    "    train_data = DataLoader(trainpath,args.data+'/ilsvrc12_train.txt',\n",
    "                            classes=args.classes)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_data,\n",
    "                                            batch_size=args.batch,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=args.cores)\n",
    "    \n",
    "    valpath = args.data+'/ILSVRC2012_img_val'\n",
    "    if os.path.exists(valpath+'_255x255'):\n",
    "        valpath += '_255x255'\n",
    "    val_data = DataLoader(valpath, args.data+'/ilsvrc12_val.txt',\n",
    "                            classes=args.classes)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_data,\n",
    "                                            batch_size=args.batch,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=args.cores)\n",
    "    N = train_data.N\n",
    "    \n",
    "    iter_per_epoch = train_data.N/args.batch\n",
    "    print('Images: train %d, validation %d'%(train_data.N,val_data.N))\n",
    "    \n",
    "    # Network initialize\n",
    "    net = Network(args.classes)\n",
    "    if args.gpu is not None:\n",
    "        net.cuda()\n",
    "    \n",
    "    ############## Load from checkpoint if exists, otherwise from model ###############\n",
    "    if os.path.exists(args.checkpoint):\n",
    "        files = [f for f in os.listdir(args.checkpoint) if 'pth' in f]\n",
    "        if len(files)>0:\n",
    "            files.sort()\n",
    "            #print files\n",
    "            ckp = files[-1]\n",
    "            net.load_state_dict(torch.load(args.checkpoint+'/'+ckp))\n",
    "            args.iter_start = int(ckp.split(\".\")[-3].split(\"_\")[-1])\n",
    "            print('Starting from: ',ckp)\n",
    "        else:\n",
    "            if args.model is not None:\n",
    "                net.load(args.model)\n",
    "    else:\n",
    "        if args.model is not None:\n",
    "            net.load(args.model)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(net.parameters(),lr=args.lr,momentum=0.9,weight_decay = 5e-4)\n",
    "    \n",
    "    logger = Logger(args.checkpoint+'/train')\n",
    "    logger_test = Logger(args.checkpoint+'/test')\n",
    "    \n",
    "    ############## TESTING ###############\n",
    "    if args.evaluate:\n",
    "        test(net,criterion,None,val_loader,0)\n",
    "        return\n",
    "    \n",
    "    ############## TRAINING ###############\n",
    "    print(('Start training: lr %f, batch size %d, classes %d'%(args.lr,args.batch,args.classes)))\n",
    "    print(('Checkpoint: '+args.checkpoint))\n",
    "    \n",
    "    # Train the Model\n",
    "    batch_time, net_time = [], []\n",
    "    steps = args.iter_start\n",
    "    for epoch in range(int(args.iter_start/iter_per_epoch),args.epochs):\n",
    "        if epoch%10==0 and epoch>0:\n",
    "            test(net,criterion,logger_test,val_loader,steps)\n",
    "        lr = adjust_learning_rate(optimizer, epoch, init_lr=args.lr, step=20, decay=0.1)\n",
    "        \n",
    "        end = time()\n",
    "        for i, (images, labels, original) in enumerate(train_loader):\n",
    "            batch_time.append(time()-end)\n",
    "            if len(batch_time)>100:\n",
    "                del batch_time[0]\n",
    "            \n",
    "            images = Variable(images)\n",
    "            labels = Variable(labels)\n",
    "            if args.gpu is not None:\n",
    "                images = images.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()\n",
    "            t = time()\n",
    "            outputs = net(images)\n",
    "            net_time.append(time()-t)\n",
    "            if len(net_time)>100:\n",
    "                del net_time[0]\n",
    "            \n",
    "            prec1, prec5 = compute_accuracy(outputs.cpu().data, labels.cpu().data, topk=(1, 5))\n",
    "            acc = prec1[0]\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss = float(loss.cpu().data.numpy())\n",
    "\n",
    "            if steps%20==0:\n",
    "                print(('[%2d/%2d] %5d) [batch load % 2.3fsec, net %1.2fsec], LR %.5f, Loss: % 1.3f, Accuracy % 2.2f%%' %(\n",
    "                            epoch+1, args.epochs, steps, \n",
    "                            np.mean(batch_time), np.mean(net_time),\n",
    "                            lr, loss,acc)))\n",
    "\n",
    "            if steps%20==0:\n",
    "                logger.scalar_summary('accuracy', acc, steps)\n",
    "                logger.scalar_summary('loss', loss, steps)\n",
    "                \n",
    "                original = [im[0] for im in original]\n",
    "                imgs = np.zeros([9,75,75,3])\n",
    "                for ti, img in enumerate(original):\n",
    "                    img = img.numpy()\n",
    "                    imgs[ti] = np.stack([(im-im.min())/(im.max()-im.min()) \n",
    "                                         for im in img],axis=2)\n",
    "                \n",
    "                logger.image_summary('input', imgs, steps)\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "            if steps%1000==0:\n",
    "                filename = '%s/jps_%03i_%06d.pth.tar'%(args.checkpoint,epoch,steps)\n",
    "                net.save(filename)\n",
    "                print('Saved: '+args.checkpoint)\n",
    "            \n",
    "            end = time()\n",
    "\n",
    "        if os.path.exists(args.checkpoint+'/stop.txt'):\n",
    "            # break without using CTRL+C\n",
    "            break\n",
    "\n",
    "def test(net,criterion,logger,val_loader,steps):\n",
    "    print('Evaluating network.......')\n",
    "    accuracy = []\n",
    "    net.eval()\n",
    "    for i, (images, labels, _) in enumerate(val_loader):\n",
    "        images = Variable(images)\n",
    "        if args.gpu is not None:\n",
    "            images = images.cuda()\n",
    "\n",
    "        # Forward + Backward + Optimize\n",
    "        outputs = net(images)\n",
    "        outputs = outputs.cpu().data\n",
    "\n",
    "        prec1, prec5 = compute_accuracy(outputs, labels, topk=(1, 5))\n",
    "        accuracy.append(prec1[0])\n",
    "\n",
    "    if logger is not None:\n",
    "        logger.scalar_summary('accuracy', np.mean(accuracy), steps)\n",
    "    print('TESTING: %d), Accuracy %.2f%%' %(steps,np.mean(accuracy)))\n",
    "    net.train()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DataLoader initialize ILSVRC2012_train_processed\n",
    "\n",
    "data_path = '/Users/liujiayao/Desktop/1008 Deep Learning/Project/PyTorch-YOLOv3/data/custom/images'\n",
    "txt_list = '/Users/liujiayao/Desktop/1008 Deep Learning/Project/PyTorch-YOLOv3/data/custom/train.txt'\n",
    "\n",
    "    trainpath = args.data+'/ILSVRC2012_img_train'\n",
    "    if os.path.exists(trainpath+'_255x255'):\n",
    "        trainpath += '_255x255'\n",
    "    train_data = DataLoader(trainpath,args.data+'/ilsvrc12_train.txt',\n",
    "                            classes=args.classes)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_data,\n",
    "                                            batch_size=args.batch,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=args.cores)\n",
    "    \n",
    "    valpath = args.data+'/ILSVRC2012_img_val'\n",
    "    if os.path.exists(valpath+'_255x255'):\n",
    "        valpath += '_255x255'\n",
    "    val_data = DataLoader(valpath, args.data+'/ilsvrc12_val.txt',\n",
    "                            classes=args.classes)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_data,\n",
    "                                            batch_size=args.batch,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=args.cores)\n",
    "    N = train_data.N\n",
    "    \n",
    "    iter_per_epoch = train_data.N/args.batch\n",
    "    print('Images: train %d, validation %d'%(train_data.N,val_data.N))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x138e0f5c0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/liujiayao/anaconda3/envs/nlp_env/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n",
      "    # Send something to pin_memory_thread in case it is waiting\n",
      "  File \"/Users/liujiayao/anaconda3/envs/nlp_env/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n",
      "    \n",
      "  File \"/Users/liujiayao/anaconda3/envs/nlp_env/lib/python3.6/multiprocessing/process.py\", line 124, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/Users/liujiayao/anaconda3/envs/nlp_env/lib/python3.6/multiprocessing/popen_fork.py\", line 50, in wait\n",
      "    return self.poll(os.WNOHANG if timeout == 0.0 else 0)\n",
      "  File \"/Users/liujiayao/anaconda3/envs/nlp_env/lib/python3.6/multiprocessing/popen_fork.py\", line 28, in poll\n",
      "    pid, sts = os.waitpid(self.pid, flag)\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Aug 18 11:58:07 2017\n",
    "\n",
    "@author: Biagio Brattoli\n",
    "\"\"\"\n",
    "import os, numpy as np\n",
    "from time import time\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from random import shuffle\n",
    "\n",
    "def load_image(path,permutations,image_transformer,augment_tile):\n",
    "    img = Image.open(path).convert('RGB')\n",
    "    img = image_transformer(img)\n",
    "    \n",
    "    a = 75/2\n",
    "    tiles = [None] * 9\n",
    "    for n in range(9):\n",
    "        i = n/3\n",
    "        j = n%3\n",
    "        c = [a*i*2+a,a*j*2+a]\n",
    "        tile = img.crop((c[1]-a,c[0]-a,c[1]+a+1,c[0]+a+1))\n",
    "        tile = augment_tile(tile)\n",
    "        # Normalize the patches indipendently to avoid low level features shortcut\n",
    "        #m = tile.mean()\n",
    "        #s = tile.std()\n",
    "        #norm = transforms.Normalize(mean=[m, m, m],\n",
    "                                    #std =[s, s, s])\n",
    "        #tile = norm(tile)\n",
    "        tiles[n] = tile\n",
    "    \n",
    "    order = np.random.randint(len(permutations))\n",
    "    data = [tiles[permutations[order][t]] for t in range(9)]\n",
    "    data = torch.stack(data,0)\n",
    "    return data, int(order)\n",
    "\n",
    "class DataLoader(data.Dataset):\n",
    "    def __init__(self, data_path, txt_list, classes=1000):\n",
    "        self.data_path = data_path\n",
    "        self.names = self.__dataset_info(txt_list)\n",
    "        self.N = len(self.names)\n",
    "        self.permutations = self.__retrive_permutations(classes)\n",
    "\n",
    "        self.__image_transformer = transforms.Compose([\n",
    "            transforms.Resize(256, Image.BILINEAR),\n",
    "            transforms.CenterCrop(255)])\n",
    "        self.__augment_tile = transforms.Compose([\n",
    "            transforms.RandomCrop(64),\n",
    "            transforms.Resize((75, 75), Image.BILINEAR),\n",
    "            transforms.Lambda(rgb_jittering),\n",
    "            transforms.ToTensor(),\n",
    "            # transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "            # std =[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        framename = self.data_path + '/' + self.names[index]\n",
    "\n",
    "        img = Image.open(framename).convert('RGB')\n",
    "        if np.random.rand() < 0.30:\n",
    "            img = img.convert('LA').convert('RGB')\n",
    "\n",
    "        if img.size[0] != 255:\n",
    "            img = self.__image_transformer(img)\n",
    "\n",
    "        s = float(img.size[0]) / 3\n",
    "        a = s / 2\n",
    "        tiles = [None] * 9\n",
    "        for n in range(9):\n",
    "            i = n / 3\n",
    "            j = n % 3\n",
    "            c = [a * i * 2 + a, a * j * 2 + a]\n",
    "            c = np.array([c[1] - a, c[0] - a, c[1] + a + 1, c[0] + a + 1]).astype(int)\n",
    "            tile = img.crop(c.tolist())\n",
    "            tile = self.__augment_tile(tile)\n",
    "            # Normalize the patches indipendently to avoid low level features shortcut\n",
    "            m, s = tile.view(3, -1).mean(dim=1).numpy(), tile.view(3, -1).std(dim=1).numpy()\n",
    "            s[s == 0] = 1\n",
    "            norm = transforms.Normalize(mean=m.tolist(), std=s.tolist())\n",
    "            tile = norm(tile)\n",
    "            tiles[n] = tile\n",
    "\n",
    "        order = np.random.randint(len(self.permutations))\n",
    "        data = [tiles[self.permutations[order][t]] for t in range(9)]\n",
    "        data = torch.stack(data, 0)\n",
    "\n",
    "        return data, int(order), tiles\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "\n",
    "    def __dataset_info(self, txt_labels):\n",
    "        with open(txt_labels, 'r') as f:\n",
    "            images_list = f.readlines()\n",
    "\n",
    "        file_names = []\n",
    "        labels = []\n",
    "        for row in images_list:\n",
    "            row = row.split('\\n')\n",
    "            file_names.append(row[0])\n",
    "            #labels.append(int(row[1]))\n",
    "\n",
    "        return file_names#, labels\n",
    "\n",
    "    def __retrive_permutations(self, classes):\n",
    "        all_perm = np.load('permutations_%d.npy' % (classes))\n",
    "        # from range [1,9] to [0,8]\n",
    "        if all_perm.min() == 1:\n",
    "            all_perm = all_perm - 1\n",
    "\n",
    "        return all_perm\n",
    "\n",
    "def rgb_jittering(im):\n",
    "    im = np.array(im,np.float32)#convert to numpy array\n",
    "    for ch in range(3):\n",
    "        thisRand = np.random.uniform(0.8, 1.2)\n",
    "        im[:,:,ch] *= thisRand\n",
    "    shiftVal = np.random.randint(0,6)\n",
    "    if np.random.randint(2) == 1:\n",
    "        shiftVal = -shiftVal\n",
    "    im += shiftVal;\n",
    "    im = im.astype(np.uint8)\n",
    "    im = im.astype(np.float32)\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
